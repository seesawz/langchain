{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG是LLM中最典型也是最流行的设计模式，其全称是Retrieval Augmented Generation，可以被翻译为检索增强生成技术，从标题上也能了解到其核心的流程。检索 =》 增强 =》 生成  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM的局限性\n",
    "    幻觉问题\n",
    "    对知识领域的欠缺\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG就是针对以上两点问题解决的\n",
    "\n",
    "RAG的基本流程是：\n",
    "    1. 用户输入提问\n",
    "    2. 检索：根据用户提问对向量数据库进行相似性检测，查找与回答用户问题最想关的内容\n",
    "    3. 增强：根据检索的结果，生成prompt。\n",
    "    4. 生成：将增强后的prompt传递给llm，返回数据给用户"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以 RAG 就是哪里有问题解决哪里，既然大模型无法获得最新和内部的数据集，那我们就使用外挂的向量数据库为 llm 提供最新和内部的数据库。既然大模型有幻想问题，我们就将回答问题所需要的信息和知识编码到上下文中，强制大模型只参考这些内容进行回答。\n",
    "\n",
    "RAG 更底层的逻辑是，也是我们对待 llm 正确的态度：llm 是逻辑推理引擎，而不是信息引擎。所以，由外挂的向量数据库提供最有效的知识，然后由 llm 根据知识进行推理，提供有价值的回复。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
